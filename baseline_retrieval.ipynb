{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Retrieval Pipeline\n",
    "\n",
    "This notebook replicates the functionality of `retrieval.ps1` for executing retrieval tasks.\n",
    "It performs document retrieval using BM25 or BGE-M3 retrievers and evaluates the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import custom modules\n",
    "from src.datasets.dataset import get_task_datasets\n",
    "from src.llms import Mock\n",
    "from src.tasks.retrieval import RetrievalTask\n",
    "from src.retrievers import CustomBM25Retriever, CustomBGEM3Retriever\n",
    "from src.embeddings.base import HuggingfaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  ocr_type: gt\n",
      "  retriever_type: bm25\n",
      "  model_name: mock\n",
      "  retrieve_top_k: 2\n",
      "  data_path: data/qas_v2_clean.json\n",
      "  docs_path: data/retrieval_base/gt\n",
      "  task: Retrieval\n",
      "  evaluation_stage: retrieval\n",
      "  num_threads: 1\n",
      "  show_progress_bar: True\n",
      "  output_path: ./output\n",
      "  chunk_size: 1024\n",
      "  chunk_overlap: 0\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters (matching retrieval.ps1)\n",
    "config = {\n",
    "    'ocr_type': 'gt',  # OCR type: 'gt', 'paddleocr', etc.\n",
    "    'retriever_type': 'bm25',  # Retriever type: 'bm25' or 'bge-m3'\n",
    "    'model_name': 'mock',\n",
    "    'retrieve_top_k': 2,\n",
    "    'data_path': 'data/qas_v2_clean.json',  # Using cleaned data - run 'python clean_data.py' first\n",
    "    'docs_path': None,  # Will be set based on ocr_type\n",
    "    'task': 'Retrieval',\n",
    "    'evaluation_stage': 'retrieval',\n",
    "    'num_threads': 1,\n",
    "    'show_progress_bar': True,\n",
    "    'output_path': './output',\n",
    "    'chunk_size': 1024,\n",
    "    'chunk_overlap': 0\n",
    "}\n",
    "\n",
    "# Set docs_path based on ocr_type\n",
    "config['docs_path'] = f\"data/retrieval_base/{config['ocr_type']}\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 0\n"
     ]
    }
   ],
   "source": [
    "def setup_seed(seed=0):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    import torch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(0)\n",
    "print(\"Random seed set to 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from data/qas_v2_clean.json...\n",
      "Loaded 7481 data points\n",
      "\n",
      "Sample data point:\n",
      "  doc_name: finance/JPMORGAN_2021Q1_10Q\n",
      "  ID: 00073cc2-c801-467c-9039-fca63c78c6a9\n",
      "  questions: What was the total amount of nonaccrual loans retained as of March 31, 2021?\n",
      "  answers: 842\n",
      "  doc_type: finance\n",
      "  answer_form: Numeric\n",
      "  evidence_source: table\n",
      "  evidence_context: Nonaccrual loans retained $^{(\\mathrm{a})}$ & \\$ & 842 & \\$ & 689 & $22 \\%$\n",
      "  evidence_page_no: 24\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(f\"Loading dataset from {config['data_path']}...\")\n",
    "datasets = get_task_datasets(config['data_path'], config['task'])\n",
    "dataset = datasets[0]\n",
    "print(f\"Loaded {len(dataset)} data points\")\n",
    "\n",
    "# Display a sample data point\n",
    "if len(dataset) > 0:\n",
    "    print(\"\\nSample data point:\")\n",
    "    sample = dataset[0]\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"  {key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Mock LLM\n",
      "\n",
      "Initializing bm25 retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1011/1011 [00:01<00:00, 588.33it/s]\n",
      "Parsing nodes: 100%|██████████| 1341/1341 [00:06<00:00, 216.05it/s]\n",
      "Parsing nodes: 100%|██████████| 2133/2133 [00:05<00:00, 408.20it/s]\n",
      "Parsing nodes: 100%|██████████| 1187/1187 [00:01<00:00, 857.59it/s]\n",
      "Parsing nodes: 100%|██████████| 1724/1724 [00:01<00:00, 1493.76it/s]\n",
      "Parsing nodes: 100%|██████████| 487/487 [00:02<00:00, 205.14it/s]\n",
      "Parsing nodes: 100%|██████████| 288/288 [00:00<00:00, 1892.83it/s]\n",
      "Parsing nodes: 100%|██████████| 204/204 [00:00<00:00, 304.32it/s]\n",
      "Parsing nodes: 100%|██████████| 679/679 [00:00<00:00, 994.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing finished for all directories!\n",
      "Retriever initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the mock LLM (not used in retrieval stage but required by the pipeline)\n",
    "llm = Mock()\n",
    "print(\"Initialized Mock LLM\")\n",
    "\n",
    "# Initialize the retriever based on configuration\n",
    "print(f\"\\nInitializing {config['retriever_type']} retriever...\")\n",
    "if config['retriever_type'] == \"bge-m3\":\n",
    "    embed_model = HuggingfaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "    retriever = CustomBGEM3Retriever(\n",
    "        config['docs_path'], \n",
    "        embed_model=embed_model, \n",
    "        embed_dim=1024,\n",
    "        chunk_size=config['chunk_size'], \n",
    "        chunk_overlap=config['chunk_overlap'], \n",
    "        similarity_top_k=config['retrieve_top_k']\n",
    "    )\n",
    "elif config['retriever_type'] == \"bm25\":\n",
    "    retriever = CustomBM25Retriever(\n",
    "        config['docs_path'], \n",
    "        chunk_size=config['chunk_size'], \n",
    "        chunk_overlap=config['chunk_overlap'], \n",
    "        similarity_top_k=config['retrieve_top_k']\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported retriever type: {config['retriever_type']}\")\n",
    "\n",
    "print(f\"Retriever initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Retrieval Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval task initialized with output directory: ./output\\retrieval\\gt\n"
     ]
    }
   ],
   "source": [
    "# Initialize the retrieval task\n",
    "output_dir = os.path.join(config['output_path'], config['evaluation_stage'], config['ocr_type'])\n",
    "task = RetrievalTask(output_dir=output_dir)\n",
    "task.set_model(llm, retriever)\n",
    "print(f\"Retrieval task initialized with output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execute Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 7481 data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving: 100%|██████████| 7481/7481 [05:22<00:00, 23.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 7481 data points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each data point\n",
    "results = []\n",
    "\n",
    "print(f\"\\nProcessing {len(dataset)} data points...\")\n",
    "for data_point in tqdm(dataset, desc=\"Retrieving\", disable=not config['show_progress_bar']):\n",
    "    try:\n",
    "        # Perform retrieval\n",
    "        retrieval_results = task.retrieve_docs(data_point)\n",
    "        data_point[\"retrieval_results\"] = retrieval_results\n",
    "        \n",
    "        # Score the retrieval\n",
    "        result = {'id': data_point['ID'], **task.scoring(data_point)}\n",
    "        results.append(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error processing data point {data_point.get('ID', 'unknown')}: {e}\")\n",
    "        data_point[\"retrieval_results\"] = []\n",
    "        result = {'id': data_point['ID'], **task.scoring(data_point)}\n",
    "        results.append(result)\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid results: 7481 out of 7481\n",
      "\n",
      "Overall Metrics:\n",
      "  avg. lcs: 0.8031\n",
      "  num: 7481\n"
     ]
    }
   ],
   "source": [
    "# Filter valid results\n",
    "valid_results = [result for result in results if result['valid']]\n",
    "print(f\"Valid results: {len(valid_results)} out of {len(results)}\")\n",
    "\n",
    "# Compute overall metrics\n",
    "if len(valid_results) > 0:\n",
    "    overall = task.compute_overall(valid_results)\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    for key, value in overall.items():\n",
    "        print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "else:\n",
    "    overall = {}\n",
    "    print(\"No valid results to compute metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: ./output\\retrieval\\gt\\all_bm25_top2.json\n"
     ]
    }
   ],
   "source": [
    "# Prepare output\n",
    "info = {\n",
    "    'task': task.__class__.__name__, \n",
    "    'retriever': retriever.__class__.__name__,\n",
    "    'ocr_type': config['ocr_type'],\n",
    "    'retrieve_top_k': config['retrieve_top_k'],\n",
    "    'chunk_size': config['chunk_size'],\n",
    "    'chunk_overlap': config['chunk_overlap']\n",
    "}\n",
    "\n",
    "output = {\n",
    "    'info': info,\n",
    "    'overall': overall,\n",
    "    'results': results\n",
    "}\n",
    "\n",
    "# Define output path\n",
    "ret_name = {\n",
    "    \"CustomBM25Retriever\": \"bm25\",\n",
    "    \"CustomBGEM3Retriever\": \"bge-m3\"\n",
    "}[retriever.__class__.__name__]\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, f'all_{ret_name}_top{config[\"retrieve_top_k\"]}.json')\n",
    "\n",
    "# Save to JSON file\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results DataFrame shape: (7481, 9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ocr_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "retriever",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "domain",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "doc_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "evidence_source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer_form",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lcs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "valid",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "f9e23575-a53b-4849-8f3f-1110f1073f0e",
       "rows": [
        [
         "0",
         "00073cc2-c801-467c-9039-fca63c78c6a9",
         "gt",
         "bm25",
         "finance",
         "JPMORGAN_2021Q1_10Q",
         "table",
         "Numeric",
         "0.0",
         "True"
        ],
        [
         "1",
         "000b6710-f8b4-4dd4-9913-90c7d424fccf",
         "gt",
         "bm25",
         "finance",
         "JPMORGAN_2021Q1_10Q",
         "table",
         "Numeric",
         "0.0",
         "True"
        ],
        [
         "2",
         "00183cfe-ceb0-4220-b984-f33f61c61ae4",
         "gt",
         "bm25",
         "finance",
         "JPMORGAN_2021Q1_10Q",
         "table",
         "Numeric",
         "0.0",
         "True"
        ],
        [
         "3",
         "002f9cc4-096b-4aff-b5b7-751f497e28aa",
         "gt",
         "bm25",
         "finance",
         "JPMORGAN_2021Q1_10Q",
         "table",
         "Numeric",
         "1.0",
         "True"
        ],
        [
         "4",
         "003c6ab8-2d19-4cf0-8d43-8259815f9e34",
         "gt",
         "bm25",
         "finance",
         "JPMORGAN_2021Q1_10Q",
         "table",
         "Numeric",
         "0.125",
         "True"
        ],
        [
         "5",
         "0042d740-0c34-439f-ad44-e0f06a9e72f8",
         "gt",
         "bm25",
         "finance",
         "JPMORGAN_2021Q1_10Q",
         "table",
         "Numeric",
         "1.0",
         "True"
        ],
        [
         "6",
         "004cdaf0-0ed9-4a32-8f0f-a9db4b6a3fea",
         "gt",
         "bm25",
         "finance",
         "JPMORGAN_2021Q1_10Q",
         "text",
         "Numeric",
         "1.0",
         "True"
        ],
        [
         "7",
         "0068eeac-7cfb-49f6-8de0-4a849afe5363",
         "gt",
         "bm25",
         "finance",
         "DUDE_026e416e05d6efc5f061a2165fd827c3",
         "text",
         "String",
         "0.9259259259259259",
         "True"
        ],
        [
         "8",
         "006baa01-fdbc-46e7-8734-baefc2e4866f",
         "gt",
         "bm25",
         "finance",
         "JPMORGAN_2021Q1_10Q",
         "table",
         "Numeric",
         "0.0",
         "True"
        ],
        [
         "9",
         "007b0a78-f278-4163-9312-8e5cbea3351d",
         "gt",
         "bm25",
         "finance",
         "JPMORGAN_2021Q1_10Q",
         "table",
         "Numeric",
         "1.0",
         "True"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ocr_type</th>\n",
       "      <th>retriever</th>\n",
       "      <th>domain</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>evidence_source</th>\n",
       "      <th>answer_form</th>\n",
       "      <th>lcs</th>\n",
       "      <th>valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00073cc2-c801-467c-9039-fca63c78c6a9</td>\n",
       "      <td>gt</td>\n",
       "      <td>bm25</td>\n",
       "      <td>finance</td>\n",
       "      <td>JPMORGAN_2021Q1_10Q</td>\n",
       "      <td>table</td>\n",
       "      <td>Numeric</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000b6710-f8b4-4dd4-9913-90c7d424fccf</td>\n",
       "      <td>gt</td>\n",
       "      <td>bm25</td>\n",
       "      <td>finance</td>\n",
       "      <td>JPMORGAN_2021Q1_10Q</td>\n",
       "      <td>table</td>\n",
       "      <td>Numeric</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00183cfe-ceb0-4220-b984-f33f61c61ae4</td>\n",
       "      <td>gt</td>\n",
       "      <td>bm25</td>\n",
       "      <td>finance</td>\n",
       "      <td>JPMORGAN_2021Q1_10Q</td>\n",
       "      <td>table</td>\n",
       "      <td>Numeric</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002f9cc4-096b-4aff-b5b7-751f497e28aa</td>\n",
       "      <td>gt</td>\n",
       "      <td>bm25</td>\n",
       "      <td>finance</td>\n",
       "      <td>JPMORGAN_2021Q1_10Q</td>\n",
       "      <td>table</td>\n",
       "      <td>Numeric</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003c6ab8-2d19-4cf0-8d43-8259815f9e34</td>\n",
       "      <td>gt</td>\n",
       "      <td>bm25</td>\n",
       "      <td>finance</td>\n",
       "      <td>JPMORGAN_2021Q1_10Q</td>\n",
       "      <td>table</td>\n",
       "      <td>Numeric</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0042d740-0c34-439f-ad44-e0f06a9e72f8</td>\n",
       "      <td>gt</td>\n",
       "      <td>bm25</td>\n",
       "      <td>finance</td>\n",
       "      <td>JPMORGAN_2021Q1_10Q</td>\n",
       "      <td>table</td>\n",
       "      <td>Numeric</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>004cdaf0-0ed9-4a32-8f0f-a9db4b6a3fea</td>\n",
       "      <td>gt</td>\n",
       "      <td>bm25</td>\n",
       "      <td>finance</td>\n",
       "      <td>JPMORGAN_2021Q1_10Q</td>\n",
       "      <td>text</td>\n",
       "      <td>Numeric</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068eeac-7cfb-49f6-8de0-4a849afe5363</td>\n",
       "      <td>gt</td>\n",
       "      <td>bm25</td>\n",
       "      <td>finance</td>\n",
       "      <td>DUDE_026e416e05d6efc5f061a2165fd827c3</td>\n",
       "      <td>text</td>\n",
       "      <td>String</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>006baa01-fdbc-46e7-8734-baefc2e4866f</td>\n",
       "      <td>gt</td>\n",
       "      <td>bm25</td>\n",
       "      <td>finance</td>\n",
       "      <td>JPMORGAN_2021Q1_10Q</td>\n",
       "      <td>table</td>\n",
       "      <td>Numeric</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>007b0a78-f278-4163-9312-8e5cbea3351d</td>\n",
       "      <td>gt</td>\n",
       "      <td>bm25</td>\n",
       "      <td>finance</td>\n",
       "      <td>JPMORGAN_2021Q1_10Q</td>\n",
       "      <td>table</td>\n",
       "      <td>Numeric</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id ocr_type retriever   domain  \\\n",
       "0  00073cc2-c801-467c-9039-fca63c78c6a9       gt      bm25  finance   \n",
       "1  000b6710-f8b4-4dd4-9913-90c7d424fccf       gt      bm25  finance   \n",
       "2  00183cfe-ceb0-4220-b984-f33f61c61ae4       gt      bm25  finance   \n",
       "3  002f9cc4-096b-4aff-b5b7-751f497e28aa       gt      bm25  finance   \n",
       "4  003c6ab8-2d19-4cf0-8d43-8259815f9e34       gt      bm25  finance   \n",
       "5  0042d740-0c34-439f-ad44-e0f06a9e72f8       gt      bm25  finance   \n",
       "6  004cdaf0-0ed9-4a32-8f0f-a9db4b6a3fea       gt      bm25  finance   \n",
       "7  0068eeac-7cfb-49f6-8de0-4a849afe5363       gt      bm25  finance   \n",
       "8  006baa01-fdbc-46e7-8734-baefc2e4866f       gt      bm25  finance   \n",
       "9  007b0a78-f278-4163-9312-8e5cbea3351d       gt      bm25  finance   \n",
       "\n",
       "                                doc_name evidence_source answer_form  \\\n",
       "0                    JPMORGAN_2021Q1_10Q           table     Numeric   \n",
       "1                    JPMORGAN_2021Q1_10Q           table     Numeric   \n",
       "2                    JPMORGAN_2021Q1_10Q           table     Numeric   \n",
       "3                    JPMORGAN_2021Q1_10Q           table     Numeric   \n",
       "4                    JPMORGAN_2021Q1_10Q           table     Numeric   \n",
       "5                    JPMORGAN_2021Q1_10Q           table     Numeric   \n",
       "6                    JPMORGAN_2021Q1_10Q            text     Numeric   \n",
       "7  DUDE_026e416e05d6efc5f061a2165fd827c3            text      String   \n",
       "8                    JPMORGAN_2021Q1_10Q           table     Numeric   \n",
       "9                    JPMORGAN_2021Q1_10Q           table     Numeric   \n",
       "\n",
       "        lcs  valid  \n",
       "0  0.000000   True  \n",
       "1  0.000000   True  \n",
       "2  0.000000   True  \n",
       "3  1.000000   True  \n",
       "4  0.125000   True  \n",
       "5  1.000000   True  \n",
       "6  1.000000   True  \n",
       "7  0.925926   True  \n",
       "8  0.000000   True  \n",
       "9  1.000000   True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame for visualization\n",
    "results_df = []\n",
    "\n",
    "# Load QA data for additional context\n",
    "with open(config['data_path'], 'r', encoding='utf-8') as f:\n",
    "    qa_dict = {item['ID']: item for item in json.load(f)}\n",
    "\n",
    "for result in results:\n",
    "    if result['id'] in qa_dict:\n",
    "        qa_item = qa_dict[result['id']]\n",
    "        results_df.append({\n",
    "            'id': result['id'],\n",
    "            'ocr_type': config['ocr_type'],\n",
    "            'retriever': ret_name,\n",
    "            'domain': qa_item.get('doc_type', ''),\n",
    "            'doc_name': qa_item.get('doc_name', '').split('/')[-1],\n",
    "            'evidence_source': qa_item.get('evidence_source', ''),\n",
    "            'answer_form': qa_item.get('answer_form', ''),\n",
    "            'lcs': result['metrics']['lcs'],\n",
    "            'valid': result['valid']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results_df)\n",
    "print(f\"\\nResults DataFrame shape: {df.shape}\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Results Analysis by Evidence Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results by Evidence Source:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "evidence_source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "('lcs_percent', 'mean')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('lcs_percent', 'count')",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "('valid', 'sum')",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "595721e3-1fb1-4cdf-ba86-f2c188f031e2",
       "rows": [
        [
         "chart",
         "71.1",
         "747",
         "747"
        ],
        [
         "formula",
         "80.61",
         "1142",
         "1142"
        ],
        [
         "multi",
         "66.42",
         "126",
         "126"
        ],
        [
         "reading_order",
         "76.92",
         "52",
         "52"
        ],
        [
         "table",
         "75.9",
         "2053",
         "2053"
        ],
        [
         "text",
         "85.54",
         "3361",
         "3361"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">lcs_percent</th>\n",
       "      <th>valid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evidence_source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chart</th>\n",
       "      <td>71.10</td>\n",
       "      <td>747</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formula</th>\n",
       "      <td>80.61</td>\n",
       "      <td>1142</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi</th>\n",
       "      <td>66.42</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reading_order</th>\n",
       "      <td>76.92</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>table</th>\n",
       "      <td>75.90</td>\n",
       "      <td>2053</td>\n",
       "      <td>2053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>85.54</td>\n",
       "      <td>3361</td>\n",
       "      <td>3361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                lcs_percent       valid\n",
       "                       mean count   sum\n",
       "evidence_source                        \n",
       "chart                 71.10   747   747\n",
       "formula               80.61  1142  1142\n",
       "multi                 66.42   126   126\n",
       "reading_order         76.92    52    52\n",
       "table                 75.90  2053  2053\n",
       "text                  85.54  3361  3361"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Average LCS: 80.31%\n",
      "Total Valid Results: 7481 / 7481\n"
     ]
    }
   ],
   "source": [
    "# Analyze results by evidence source\n",
    "if len(df) > 0:\n",
    "    df['lcs_percent'] = df['lcs'] * 100\n",
    "    \n",
    "    # Group by evidence source\n",
    "    evidence_summary = df.groupby('evidence_source').agg({\n",
    "        'lcs_percent': ['mean', 'count'],\n",
    "        'valid': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nResults by Evidence Source:\")\n",
    "    display(evidence_summary)\n",
    "    \n",
    "    # Overall average\n",
    "    overall_avg = df['lcs_percent'].mean()\n",
    "    print(f\"\\nOverall Average LCS: {overall_avg:.2f}%\")\n",
    "    print(f\"Total Valid Results: {df['valid'].sum()} / {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results Analysis by Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results by Domain:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "domain",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "('lcs_percent', 'mean')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('lcs_percent', 'count')",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "('valid', 'sum')",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "2f3f0b96-3f06-465d-aff7-1d398260abab",
       "rows": [
        [
         "academic",
         "80.63",
         "1150",
         "1150"
        ],
        [
         "administration",
         "84.8",
         "1322",
         "1322"
        ],
        [
         "finance",
         "62.29",
         "1365",
         "1365"
        ],
        [
         "law",
         "85.81",
         "1142",
         "1142"
        ],
        [
         "manual",
         "84.54",
         "1107",
         "1107"
        ],
        [
         "news",
         "87.85",
         "546",
         "546"
        ],
        [
         "textbook",
         "84.13",
         "849",
         "849"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">lcs_percent</th>\n",
       "      <th>valid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>academic</th>\n",
       "      <td>80.63</td>\n",
       "      <td>1150</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>administration</th>\n",
       "      <td>84.80</td>\n",
       "      <td>1322</td>\n",
       "      <td>1322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>62.29</td>\n",
       "      <td>1365</td>\n",
       "      <td>1365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>law</th>\n",
       "      <td>85.81</td>\n",
       "      <td>1142</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manual</th>\n",
       "      <td>84.54</td>\n",
       "      <td>1107</td>\n",
       "      <td>1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>87.85</td>\n",
       "      <td>546</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textbook</th>\n",
       "      <td>84.13</td>\n",
       "      <td>849</td>\n",
       "      <td>849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lcs_percent       valid\n",
       "                      mean count   sum\n",
       "domain                                \n",
       "academic             80.63  1150  1150\n",
       "administration       84.80  1322  1322\n",
       "finance              62.29  1365  1365\n",
       "law                  85.81  1142  1142\n",
       "manual               84.54  1107  1107\n",
       "news                 87.85   546   546\n",
       "textbook             84.13   849   849"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze results by domain\n",
    "if len(df) > 0:\n",
    "    domain_summary = df.groupby('domain').agg({\n",
    "        'lcs_percent': ['mean', 'count'],\n",
    "        'valid': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nResults by Domain:\")\n",
    "    display(domain_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Sample Retrieval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Retrieval Results (Top 3):\n",
      "\n",
      "================================================================================\n",
      "Result 1 - ID: 00073cc2-c801-467c-9039-fca63c78c6a9\n",
      "Valid: True\n",
      "LCS Score: 0.0000\n",
      "\n",
      "Question: What was the total amount of nonaccrual loans retained as of March 31, 2021?\n",
      "\n",
      "Evidence Source: table\n",
      "\n",
      "Retrieved 2 documents:\n",
      "\n",
      "  Document 1:\n",
      "    File: JPMORGAN_2021Q1_10Q\n",
      "    Page: 28\n",
      "    Text: Selected metrics (continued)\n",
      "\\begin{tabular}{|c|c|c|c|c|c|}\n",
      "  \\multirow[b]{2}{*}{(in millions, except ratios)} & \\multicolumn{5}{|c|}{As of or for the three months ended March 31,} \\\\\n",
      "  & \\multicolumn...\n",
      "\n",
      "  Document 2:\n",
      "    File: JPMORGAN_2021Q1_10Q\n",
      "    Page: 49\n",
      "    Text: \\begin{tabular}{|c|c|c|c|c|}\n",
      "  (in millions) & & $$\\operatorname{arch} 31, 2021$$ & \\multicolumn{2}{|l|}{$$\\text { December 31, } 2020$$} \\\\\n",
      "  Retained loans ${ }^{(2)}$ & \\$ & 14,943 & \\$ & 15,406 \\\\...\n",
      "\n",
      "Ground Truth Context: Nonaccrual loans retained $^{(\\mathrm{a})}$ & \\$ & 842 & \\$ & 689 & $22 \\%$\n",
      "\n",
      "================================================================================\n",
      "Result 2 - ID: 000b6710-f8b4-4dd4-9913-90c7d424fccf\n",
      "Valid: True\n",
      "LCS Score: 0.0000\n",
      "\n",
      "Question: By what percentage did the total nonperforming assets increase from March 31, 2020, to March 31, 2021?\n",
      "\n",
      "Evidence Source: table\n",
      "\n",
      "Retrieved 2 documents:\n",
      "\n",
      "  Document 1:\n",
      "    File: JPMORGAN_2021Q1_10Q\n",
      "    Page: 91\n",
      "    Text: (a) Level 3 assets at fair value as a percentage of total Firm assets at fair value (including assets measured at fair value on a nonrecurring basis) were $1 \\%$ at both March 31, 2021 and December 31...\n",
      "\n",
      "  Document 2:\n",
      "    File: JPMORGAN_2021Q1_10Q\n",
      "    Page: 4\n",
      "    Text: - The provision for credit losses was a net benefit of $\\$ 4.2$ billion driven by net reductions in the allowance for credit losses of $\\$ 5.2$ billion, compared to an expense of $\\$ 8.3$ billion in t...\n",
      "\n",
      "Ground Truth Context: Total nonperforming assets & \\$ & 2,489 & \\$ & 1,583 & 57\n",
      "\n",
      "================================================================================\n",
      "Result 3 - ID: 00183cfe-ceb0-4220-b984-f33f61c61ae4\n",
      "Valid: True\n",
      "LCS Score: 0.0000\n",
      "\n",
      "Question: What was the allowance for loan losses to nonaccrual loans retained ratio as of March 31, 2021?\n",
      "\n",
      "Evidence Source: table\n",
      "\n",
      "Retrieved 2 documents:\n",
      "\n",
      "  Document 1:\n",
      "    File: JPMORGAN_2021Q1_10Q\n",
      "    Page: 28\n",
      "    Text: Selected metrics (continued)\n",
      "\\begin{tabular}{|c|c|c|c|c|c|}\n",
      "  \\multirow[b]{2}{*}{(in millions, except ratios)} & \\multicolumn{5}{|c|}{As of or for the three months ended March 31,} \\\\\n",
      "  & \\multicolumn...\n",
      "\n",
      "  Document 2:\n",
      "    File: JPMORGAN_2021Q1_10Q\n",
      "    Page: 4\n",
      "    Text: - The provision for credit losses was a net benefit of $\\$ 4.2$ billion driven by net reductions in the allowance for credit losses of $\\$ 5.2$ billion, compared to an expense of $\\$ 8.3$ billion in t...\n",
      "\n",
      "Ground Truth Context: Allowance for loan losses to nonaccrual loans retained $^{(\\mathrm{a})}$ & & 235 & & 206 &\n"
     ]
    }
   ],
   "source": [
    "# Display sample retrieval results\n",
    "print(\"\\nSample Retrieval Results (Top 3):\")\n",
    "for i, result in enumerate(results[:3]):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Result {i+1} - ID: {result['id']}\")\n",
    "    print(f\"Valid: {result['valid']}\")\n",
    "    print(f\"LCS Score: {result['metrics']['lcs']:.4f}\")\n",
    "    \n",
    "    log = result['log']\n",
    "    print(f\"\\nQuestion: {log['quest']}\")\n",
    "    print(f\"\\nEvidence Source: {log['evidence_source']}\")\n",
    "    \n",
    "    if 'retrieval_context' in log and len(log['retrieval_context']) > 0:\n",
    "        print(f\"\\nRetrieved {len(log['retrieval_context'])} documents:\")\n",
    "        for j, doc in enumerate(log['retrieval_context']):\n",
    "            print(f\"\\n  Document {j+1}:\")\n",
    "            print(f\"    File: {doc.get('file_name', 'N/A')}\")\n",
    "            print(f\"    Page: {doc.get('page_idx', 'N/A')}\")\n",
    "            text = doc.get('text', '')\n",
    "            print(f\"    Text: {text[:200]}...\" if len(text) > 200 else f\"    Text: {text}\")\n",
    "    else:\n",
    "        print(\"\\nNo documents retrieved\")\n",
    "    \n",
    "    gt_context = log['ground_truth_context']\n",
    "    print(f\"\\nGround Truth Context: {gt_context[:200]}...\" if len(gt_context) > 200 else f\"\\nGround Truth Context: {gt_context}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
